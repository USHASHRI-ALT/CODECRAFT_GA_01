
# CODECRAFT GENERATEAI TASK 1

## Task Title: Text Generation with GPT-2

### Author: Usha

### Description:
This project demonstrates the fine-tuning of the GPT-2 model on a custom dataset for generating contextually relevant text. The goal of this task is to replicate the style and structure of the dataset, ensuring coherent output based on the given prompts.

### Files Included:
- `README.md` : Project overview and author information
- `train_gpt2.py` : Python script to fine-tune GPT-2 using Huggingface's Transformers library
- `generate_text.py` : Script to generate text using the fine-tuned model
- `sample_dataset.txt` : Sample dataset used for training
- `requirements.txt` : List of dependencies to run the project
- `output/` : Folder containing sample outputs from the model

### Notes:
- Ensure all dependencies in `requirements.txt` are installed.
- Run `train_gpt2.py` before generating text.
